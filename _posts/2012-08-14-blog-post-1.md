---
title: "Deep Residual Learning for Image Recognition"
date: 2024-10-29
permalink: /posts/2024/10/deep-residual-learning-for-image-recognition/
---

## key summery
This paper is about ResNet (Residual Network). Resnet is a model that enables efficient learning by using residual connections to solve the gradient loss problem in deep neural networks.

### Background
- Deeplearning
![Illustration of combining vision and language modalities](/images/deeplearning.jpg){: width="200px"}
To understand resnet, need to know what deep learning is. Deep learning is a method of learning data with multiple layers of neural networks, and the structure consists of input layers, hidden layers, and output layers, like above picture. The learning method mainly through supervised learning, backpropagation algorithms minimize errors and adjust weights. Features include automatic learning of complex patterns and excellent performance in image classification, speech recognition, and natural language processing.

- 
### Residual Learning Framework

The key contribution of the paper is the introduction of a *residual learning framework* to mitigate the degradation problem. In traditional deep networks, each layer directly learns a function that maps the input to the output. In contrast, the residual learning approach reformulates this by having each layer learn a residual function that is added to the input of that layer. In other words, instead of directly learning \( H(x) \), each layer learns \( F(x) = H(x) - x \), and the final output of the layer is \( F(x) + x \). This concept is illustrated in the figure below:

![Residual Learning](../images/residual-learning-block.png)

---

## Key Concepts

1. **Degradation Problem**: As network depth increases, training accuracy can degrade due to optimization challenges, even though theoretically, deeper networks should perform better.
  
2. **Residual Function**: Rather than learning a direct mapping, each layer in a residual network learns a residual mapping. This enables the model to better handle the added complexity in deeper networks.

3. **Identity Shortcut Connections**: In residual networks, identity mappings (direct connections between input and output) are used to bypass a set of layers, allowing gradients to flow more effectively and making the model easier to optimize.

---

## Experimentation and Results

The researchers conducted experiments using both the ImageNet and CIFAR-10 datasets, evaluating networks of various depths:

- **ImageNet**: Using residual networks with up to 152 layers, the model achieved remarkable results, with an error rate of 3.57% on the ImageNet test set, winning the ILSVRC 2015 classification task.
- **CIFAR-10**: The researchers also evaluated residual networks on CIFAR-10 with depths over 100 layers. The results confirmed that residual networks scale better with depth compared to traditional networks.

Below are the training error curves for different network depths:

![Training Error on CIFAR-10](../images/cifar10-training-error.png)

This figure demonstrates how residual networks handle depth increases much more effectively than their non-residual counterparts.

---

## Impact on Object Detection

Beyond image classification, deep residual networks have shown excellent generalization to other tasks such as object detection. By replacing VGG-16 with ResNet-101 in the Faster R-CNN framework, the model achieved significant performance improvements in object detection benchmarks on PASCAL VOC and COCO datasets.

---

## Conclusion

The deep residual learning framework introduced by Kaiming He et al. has revolutionized the training of very deep networks. By addressing the degradation problem through residual connections, this approach has enabled the development of models that surpass previous state-of-the-art methods in various tasks, including classification and detection. Residual networks continue to form the foundation of many modern neural architectures in computer vision and beyond.

For those interested, you can find the full paper [here](https://arxiv.org/abs/1512.03385).

---

### References

1. He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*.

image
1. Korean Institute of Information Scientists and Engineers, "Deep Learning," KIISE Website, (accessed November 1, 2024).
2. 
---
