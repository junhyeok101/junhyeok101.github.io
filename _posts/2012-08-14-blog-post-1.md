---
title: "Deep Residual Learning for Image Recognition"
date: 2024-10-29
permalink: /posts/2024/10/deep-residual-learning-for-image-recognition/
---

## key summery
This paper is about ResNet (Residual Network). Resnet is a model that enables efficient learning by using residual connections to solve the gradient loss problem in deep neural networks.

## Background
###Deeplearning
<p align="center">
  <img src="/images/deeplearning.png" width="600px" alt="Illustration of deep learning structure">
</p>

- To understand resnet, need to know what deep learning is. **Deep learning** is a method of learning data with multiple layers of neural networks, and the structure consists of an input layer, a hidden layer, and an output layer, like above image.
- Learning is done by informing deep learning of the correct answer and making it follow along, and it goes through the process of correcting the problem by *looking backward to reduce errors*.
- Through this process, the network adjusts its weight so that it approaches the correct answer more and more accurately.
  
### Residual Learning Framework

<p align="center">
  <img src="/images/cnn.png" width="600px" alt="Illustration of deep learning structure">
</p>

**CNN (Convolutionary Neural Network)** is a type of deep learning, an artificial intelligence algorithm that automatically finds important features in an image. CNN analyzes small parts of an image like the image, to recognize basic features such as *edge* or *texture*, and then learns increasingly complex shapes or patterns through multiple layers. 

---
## Abstract

### problem
<p align="center">
  <img src="/images/error.png" width="600px" alt="Illustration of deep learning structure">
</p>
In traditional deep learning, especially CNNs, the more layers, the more complex patterns can be learned, but as above, the problem of gradient loss/explosion becomes difficult to learn.

Several methods have been used to solve this problem, but **degradation problem** has occurred when the network goes over a certain depth, which degrades the performance. To solve this problem, resnet emerged.

---

## Key Concepts

1. **Degradation Problem**: As network depth increases, training accuracy can degrade due to optimization challenges, even though theoretically, deeper networks should perform better.
  
2. **Residual Function**: Rather than learning a direct mapping, each layer in a residual network learns a residual mapping. This enables the model to better handle the added complexity in deeper networks.

3. **Identity Shortcut Connections**: In residual networks, identity mappings (direct connections between input and output) are used to bypass a set of layers, allowing gradients to flow more effectively and making the model easier to optimize.

---

## Experimentation and Results

The researchers conducted experiments using both the ImageNet and CIFAR-10 datasets, evaluating networks of various depths:

- **ImageNet**: Using residual networks with up to 152 layers, the model achieved remarkable results, with an error rate of 3.57% on the ImageNet test set, winning the ILSVRC 2015 classification task.
- **CIFAR-10**: The researchers also evaluated residual networks on CIFAR-10 with depths over 100 layers. The results confirmed that residual networks scale better with depth compared to traditional networks.

Below are the training error curves for different network depths:

![Training Error on CIFAR-10](../images/cifar10-training-error.png)

This figure demonstrates how residual networks handle depth increases much more effectively than their non-residual counterparts.

---

## Impact on Object Detection

Beyond image classification, deep residual networks have shown excellent generalization to other tasks such as object detection. By replacing VGG-16 with ResNet-101 in the Faster R-CNN framework, the model achieved significant performance improvements in object detection benchmarks on PASCAL VOC and COCO datasets.

---

## Conclusion

The deep residual learning framework introduced by Kaiming He et al. has revolutionized the training of very deep networks. By addressing the degradation problem through residual connections, this approach has enabled the development of models that surpass previous state-of-the-art methods in various tasks, including classification and detection. Residual networks continue to form the foundation of many modern neural architectures in computer vision and beyond.

For those interested, you can find the full paper [here](https://arxiv.org/abs/1512.03385).

---

### References

1. He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*.

image
1. Korean Institute of Information Scientists and Engineers, "Deep Learning," KIISE Website, (accessed November 1, 2024).
2. Prabhu, Raghav. "Understanding of Convolutional Neural Network (CNN) - Deep Learning." Medium, (accessed November 1, 2024).
---
