title: "Deep Residual Learning for Image Recognition"
date: 2024-10-29
permalink: /posts/2024/10/deep-residual-learning-for-image-recognition/
---

## key summery
This paper is about ResNet (Residual Network). Resnet is a model that enables efficient learning by using residual connections to solve the gradient loss problem in deep neural networks.

## 1. Background
### Deeplearning
<p align="center">
  <img src="/images/deeplearning.png" width="600px" alt="Illustration of deep learning structure">
</p>

- To understand resnet, need to know what deep learning is. **Deep learning** is a method of learning data with multiple layers of neural networks, and the structure consists of an input layer, a hidden layer, and an output layer, like above image.
- Learning is done by informing deep learning of the correct answer and making it follow along, and it goes through the process of correcting the problem by *looking backward to reduce errors*.
- Through this process, the network adjusts its weight so that it approaches the correct answer more and more accurately.
  
### Residual Learning Framework

<p align="center">
  <img src="/images/cnn.png" width="600px" alt="Illustration of deep learning structure">
</p>

**CNN (Convolutionary Neural Network)** is a type of deep learning, an artificial intelligence algorithm that automatically finds important features in an image. CNN analyzes small parts of an image like the image, to recognize basic features such as *edge* or *texture*, and then learns increasingly complex shapes or patterns through multiple layers. 

---
## 2. Abstract

### Problem
<p align="center">
  <img src="/images/error.png" width="600px" alt="Illustration of deep learning structure">
</p>
In prior deep learning, especially CNN, the more layers, the more complex patterns can be learned, but as above, training and test error increased when the more layer has.

Several methods have been tried to solve this problem, but **degradation problem** has occurred when the layers is bigger in a certain depth, which degrades the performance. To solve this problem, **Deep Residual Learning(resnet)** emerged.

### Solution

- Residual Learning
<p align="center">
  <img src="/images/fx.png" width="600px" alt="Illustration of deep learning structure">
</p>

The key idea of ResNet is that each layer is a full mapping function. **Instead of learning H(x), let them learn the residual function F(x):=H(x)-x**

For example, if each layer learns the difference by referring to the input using the F(x) + x structure, smooth learning is possible even in deeper networks.

This structure was created using **"Short-cut Connection"**, which enables reliable learning without complex calculations or additional settings.


### Experiment & Applicability

First, it records the highest performance with a network of 152 layers of ImageNet.

Second, other datasets such as CIFAR-10 were able to maintain low error rates at depths above 100 layers.

Furthermore, it performs well in visual recognition tasks such as object detection and image segmentation such as COCO,

a variety of utilization with the following results.

> "Our 152-layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets" (He, Zhang, Ren, & Sun, 2015).

> "1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO2015 competitions" (He, Zhang, Ren, & Sun, 2015).
</p>

---
## 3. About Residual Learning

### 3.1 Residual Learning

- **Target function**: The final target function that we want to calculate is **H(x)**.

- **Existing method**: Originally, we tried to learn H(x) directly, but ResNet lets us learn **residual function : F(x) = H(x)-x*.

F(x)= H(x)‚àíx 

- **Ease of optimization**: Learning F(x) close to zero makes input and output similar (if F(X) goes 0, it means H(x)=x), making optimization easier without complex transformations by learning small differences only when needed.

- **Degradation Problem Solved**: In the F(x)+x structure, the gradient of ùë• is added as 1 during gradient calculation, helping to prevent gradient vanishing.
  
### 3.2 Identity Mapping by Shortcuts

1. Structure
As explained in 3-1, F(x): the **residual(change)** that the network needs to learn.

x: input vector

y: Output vector, desired result, equal to H(x).

F(x) is to learn only the residual, the difference between the input and target values.

**2. Short-cut connection**

**Shortcut Connection** is a key concept that makes resnet. 

It refers to a connection method in which the output of a particular layer is skipped across multiple layers and directly transferred to the next layer in a neural network. 

This method allows only gap(residue) between input and output to be learned, so, reducing the problem of gradient loss and helping to learn efficiently as the network deepens.

$$
y = \mathcal{F}(x, \{W_i\}) + W_s x
$$

Also, if the input and output dimensions are different, use the **Line Projection ùëäùë†** in the Shortcut Connection to match the dimensions.

3. So, Why we do like this?

If you understand the concept, you can ask this question. So what's good about calculating through resnet?

Mitigating the Gradient Vanishing: Shortcut Connections skip certain layers of the network and connect directly to the subsequent layer, allowing the gradient to bypass these layers and be transmitted more effectively.

Efficiency: The network learns only the residual from the input, making it more efficient than directly learning complex transformations.

Minimizing Additional Computation: Shortcut Connections enhance performance while maintaining greater computational efficiency, as they do not require extra computations or parameters.

### 3.3 Network Architectures

<p align="center">
  <img src="/images/ar.png" width="600px" alt="Illustration of deep learning structure">
</p>

Plain Network: It is a VGG-based neural network.

Mainly use 3x3 filters, equalize the number of filters when the output size is the same, double the number of filters when the output size is halved, and set the convolutional layer's stride to 2 when reducing the image

Residual Network: This is a model that adds Shortcut Connection to the Plain Network.

Connect directly when the input and output sizes are the same, and adjust in two ways when the sizes are different
a. Add 0 to increase the dimension.
b. Use ùëäùë† to adjust the dimensions.


---
## 4. Related work

Residual Representations: Utilizing residuals in image retrieval and classification is more efficient, and multi-grid methods speed up problem solving.

Short connections: ResNet always leaves short connections open to deliver all the information and solves the problem of gradient loss by learning only the residuals.

---

## 5. Experimentation and Results

The researchers conducted experiments using both the ImageNet and CIFAR-10 datasets, evaluating networks of various depths:

<p align="center">
  <img src="/images/ImageNetClassification1.png" width="600px" alt="Illustration of deep learning structure">
</p>

<p align="center">
  <img src="/images/ImageNetClassification2.png" width="600px" alt="Illustration of deep learning structure">
</p>


- **ImageNet**: Using residual networks with up to 152 layers, the model achieved remarkable results, with an error rate of 3.57% on the ImageNet test set, winning the ILSVRC 2015 classification task.

<p align="center">
  <img src="/images/ResidualNetworks2.png" width="600px" alt="Illustration of deep learning structure">
</p>

<p align="center">
  <img src="/images/ResidualNetworks1.png" width="600px" alt="Illustration of deep learning structure">
</p>

- **CIFAR-10**: The researchers also evaluated residual networks on CIFAR-10 with depths over 100 layers. The results confirmed that residual networks scale better with depth compared to traditional networks.


This figure demonstrates how residual networks handle depth increases much more effectively than their non-residual counterparts.

---

## Impact on Object Detection

Beyond image classification, deep residual networks have shown excellent generalization to other tasks such as object detection. By replacing VGG-16 with ResNet-101 in the Faster R-CNN framework, the model achieved significant performance improvements in object detection benchmarks on PASCAL VOC and COCO datasets.

---

## Conclusion

The deep residual learning framework introduced by Kaiming He et al. has revolutionized the training of very deep networks. By addressing the degradation problem through residual connections, this approach has enabled the development of models that surpass previous state-of-the-art methods in various tasks, including classification and detection. Residual networks continue to form the foundation of many modern neural architectures in computer vision and beyond.

For those interested, you can find the full paper [here](https://arxiv.org/abs/1512.03385).

---

### References

1. He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*.

image

1. Korean Institute of Information Scientists and Engineers, "Deep Learning," KIISE Website, (accessed November 1, 2024).
   
2. Prabhu, Raghav. "Understanding of Convolutional Neural Network (CNN) - Deep Learning." Medium, (accessed November 1, 2024).
---
