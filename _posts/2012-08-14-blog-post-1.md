---
title: "Deep Residual Learning for Image Recognition"
date: 2024-10-29
permalink: /posts/2024/10/deep-residual-learning-for-image-recognition/
---

## key summery
This paper is about ResNet (Residual Network). Resnet is a model that enables efficient learning by using residual connections to solve the gradient loss problem in deep neural networks.

## 1. Background
### Deeplearning
<p align="center">
  <img src="/images/deeplearning.png" width="600px" alt="Illustration of deep learning structure">
</p>

- To understand resnet, need to know what deep learning is. **Deep learning** is a method of learning data with multiple layers of neural networks, and the structure consists of an input layer, a hidden layer, and an output layer, like above image.
- Learning is done by informing deep learning of the correct answer and making it follow along, and it goes through the process of correcting the problem by *looking backward to reduce errors*.
- Through this process, the network adjusts its weight so that it approaches the correct answer more and more accurately.
  
### Residual Learning Framework

<p align="center">
  <img src="/images/cnn.png" width="600px" alt="Illustration of deep learning structure">
</p>

**CNN (Convolutionary Neural Network)** is a type of deep learning, an artificial intelligence algorithm that automatically finds important features in an image. CNN analyzes small parts of an image like the image, to recognize basic features such as *edge* or *texture*, and then learns increasingly complex shapes or patterns through multiple layers. 

---
## 2. Abstract

### Problem
<p align="center">
  <img src="/images/error.png" width="600px" alt="Illustration of deep learning structure">
</p>
In prior deep learning, especially CNN, the more layers, the more complex patterns can be learned, but as above, training and test error increased when the more layer has.

Several methods have been tried to solve this problem, but **degradation problem** has occurred when the layers is bigger in a certain depth, which degrades the performance. To solve this problem, **Deep Residual Learning(resnet)** emerged.

### Solution

- Residual Learning
<p align="center">
  <img src="/images/fx.png" width="600px" alt="Illustration of deep learning structure">
</p>

The key idea of ResNet is that each layer is a full mapping function. **Instead of learning H(x), let them learn the residual function F(x):=H(x)-x**

For example, if each layer learns the difference by referring to the input using the F(x) + x structure, smooth learning is possible even in deeper networks.

This structure was created using **"Short-cut Connection"**, which enables reliable learning without complex calculations or additional settings.


### Experiment & Applicability

First, it records the highest performance with a network of 152 layers of ImageNet.

Second, other datasets such as CIFAR-10 were able to maintain low error rates at depths above 100 layers.

Furthermore, it performs well in visual recognition tasks such as object detection and image segmentation such as COCO,

a variety of utilization with the following results.

> "Our 152-layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets" (He, Zhang, Ren, & Sun, 2015).

> "1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO2015 competitions" (He, Zhang, Ren, & Sun, 2015).
---
## 3. Residual Learning
---
## 4. Related work

Residual Representations: Utilizing residuals in image retrieval and classification is more efficient, and multi-grid methods speed up problem solving.

Short connections: ResNet always leaves short connections open to deliver all the information and solves the problem of gradient loss by learning only the residuals.

---
## Key Concepts

1. **Degradation Problem**: As network depth increases, training accuracy can degrade due to optimization challenges, even though theoretically, deeper networks should perform better.
  
2. **Residual Function**: Rather than learning a direct mapping, each layer in a residual network learns a residual mapping. This enables the model to better handle the added complexity in deeper networks.

3. **Identity Shortcut Connections**: In residual networks, identity mappings (direct connections between input and output) are used to bypass a set of layers, allowing gradients to flow more effectively and making the model easier to optimize.

---

## Experimentation and Results

The researchers conducted experiments using both the ImageNet and CIFAR-10 datasets, evaluating networks of various depths:

- **ImageNet**: Using residual networks with up to 152 layers, the model achieved remarkable results, with an error rate of 3.57% on the ImageNet test set, winning the ILSVRC 2015 classification task.
- **CIFAR-10**: The researchers also evaluated residual networks on CIFAR-10 with depths over 100 layers. The results confirmed that residual networks scale better with depth compared to traditional networks.

Below are the training error curves for different network depths:

![Training Error on CIFAR-10](../images/cifar10-training-error.png)

This figure demonstrates how residual networks handle depth increases much more effectively than their non-residual counterparts.

---

## Impact on Object Detection

Beyond image classification, deep residual networks have shown excellent generalization to other tasks such as object detection. By replacing VGG-16 with ResNet-101 in the Faster R-CNN framework, the model achieved significant performance improvements in object detection benchmarks on PASCAL VOC and COCO datasets.

---

## Conclusion

The deep residual learning framework introduced by Kaiming He et al. has revolutionized the training of very deep networks. By addressing the degradation problem through residual connections, this approach has enabled the development of models that surpass previous state-of-the-art methods in various tasks, including classification and detection. Residual networks continue to form the foundation of many modern neural architectures in computer vision and beyond.

For those interested, you can find the full paper [here](https://arxiv.org/abs/1512.03385).

---

### References

1. He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*.

image
1. Korean Institute of Information Scientists and Engineers, "Deep Learning," KIISE Website, (accessed November 1, 2024).
2. Prabhu, Raghav. "Understanding of Convolutional Neural Network (CNN) - Deep Learning." Medium, (accessed November 1, 2024).
---
